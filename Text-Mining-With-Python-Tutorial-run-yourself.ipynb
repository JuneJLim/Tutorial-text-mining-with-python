{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining With Python - Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we will cover today:\n",
    "* Scraping text data from the Internet\n",
    "* Cleaning and preprocessing the text data for analysis\n",
    "* Making data visualizations with the cleaned/preprocessed text data that can give us interesting insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The First Jupyter Notebook Cell: Installing Word Cloud Library\n",
    "\n",
    "Select the cell below by clicking anywhere in the cell. Click the \"▶ Run\" button above or press Shift+Enter.\n",
    "\n",
    "The cell will run and install wordcloud library that we will use at the end of this tutorial. You should have all the other libraries we need already if you have downloaded Anaconda following the instruction on the GitHub page (where you downloaded this Jupyter Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # for setting up the install location\n",
    "\n",
    "!{sys.executable} -m pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Our ultimate goal is to understand the differences between two major political parties in the US – the Republican party and the Democratic party. What are the things that each party values more? In what language do they express their positions? How have they changed over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the Data\n",
    "\n",
    "### Party Platforms\n",
    "Party platforms are statements about where the parties stand on a variety of issues. In the US, new ones are written every four years, usually a few months before the upcoming presidential election. They are useful texts to understand the parties’ positions and how they are changing. ([2020 Democratic Party Platform](https://democrats.org/where-we-stand/party-platform/)) We may be able to discover something interesting by comparing the platforms between the parties and between different time periods.\n",
    "\n",
    "### The American Presidency Project\n",
    "[The American Presidency Project](https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/party-platforms), maintained by UC Santa Barbara, has archived all the party platforms published in the US and made them publicly available for research purposes. There are 100 party platforms in total, and we will scrape all of them.\n",
    "\n",
    "Let's try to scrape one party platform first. I wrote a function below that receives from us a URL to a party platform document and gives us the body text along with the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_american_presidency_doc_content(url):\n",
    "    '''\n",
    "    Extract and returns text and title from a party platform document listed in \n",
    "    https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/party-platforms.\n",
    "    \n",
    "    Parameters\n",
    "        url: string, the URL of the webpage\n",
    "    \n",
    "    Returns     \n",
    "        title: string, the title of the party platform\n",
    "        text: string, the text of the party platform\n",
    "    '''\n",
    "    page_source = requests.get(url).text\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "    text = []\n",
    "    for p in soup.find('div', class_=\"field-docs-content\").find_all('p'):\n",
    "        text.append(p.get_text(\" \"))\n",
    "    \n",
    "    # The title looks like either\n",
    "    # \"2016 Democratic Party Platform | The American Presidency Project\" or\n",
    "    # \"Republican Party Platform of 1964 | The American Presidency Project\".\n",
    "    # First, I remove the name of the website after the \"|\" symbol.\n",
    "    # (e.g., \"2016 Democratic Party Platform | The American Presidency Project\" becomes\n",
    "    #        \"2016 Democratic Party Platform\")\n",
    "    title = soup.find('title').text[:-34]\n",
    "    \n",
    "    # Second, I make the titles consistent by bringing the year to the beginning if it is at the end.\n",
    "    # (e.g., \"Republican Party Platform of 1964\" becomes \"1964 Republican Party Platform\")\n",
    "    #        \"2016 Democratic Party Platform\" stays as it is)\n",
    "    if title[-1].isdigit():\n",
    "        title = title[-4:]+' '+title[:-8]\n",
    "    \n",
    "    print('Processed:', title, url)\n",
    "    \n",
    "    return title, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm testing the function by scraping the 2016 Democratic Party Platform below. It should give me the title first, and the body text following it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrape_american_presidency_doc_content('https://www.presidency.ucsb.edu/documents/2020-democratic-party-platform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to scrape all the party platforms. To do so, we have to run this function for 100 times with 100 different URLs. Copying and pasting 100 URLs manually sound like a daunting task. Can't the computer do that for us as well? The function below scrapes all the hyperlinks to the party platform documents listed in one web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_american_presidency_doc_urls(url):\n",
    "    '''\n",
    "    Extract and returns hyperlinks to party platform documents from \n",
    "    https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/party-platforms.\n",
    "    \n",
    "    Parameters\n",
    "        url: string, the URL of the webpage\n",
    "    \n",
    "    Returns     \n",
    "        hyperlinks: list, the list of hyperlinks scraped\n",
    "    '''\n",
    "    \n",
    "    page_source = requests.get(url).text\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "\n",
    "    hyperlinks = []\n",
    "    for div in soup.find_all('div', class_=\"field-title\"):\n",
    "        for a in div.find_all('a'):\n",
    "            hyperlinks.append(a['href'])\n",
    "            \n",
    "    print('Processed:', url)\n",
    "            \n",
    "    return hyperlinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this function with the first page that lists ten most recent party platforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_american_presidency_doc_urls('https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/party-platforms?page=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One page contains ten hyperlinks, and we have ten pages in total. The page number ranges from 0 to 9. With this information, we cane scrape all the hyperlinks we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_head = 'https://www.presidency.ucsb.edu/documents/app-categories/elections-and-transitions/party-platforms?page='\n",
    "hyperlinks = []\n",
    "for page_num in range(0, 10):\n",
    "    hyperlinks += scrape_american_presidency_doc_urls(url_head + str(page_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many URLs are in the list? If my code scraped all the hyperlinks, there should be 100 URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyperlinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the first URL in the list? It should be the link to the most recent party platform in this database - '/documents/resolution-regarding-the-republican-party-platform'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the last URL in the list? It should be the oldest party platform in this database - '/documents/1840-democratic-party-platform'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlinks[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of URLs we now have, we can finally scrape all the party platforms. It can take a couple of minutes, so I skip this cell in the tutorial video. If you want to run it yourself, remove the triple quotes at the beginning and at the end before running it.\n",
    "\n",
    "After scraping all the data, I \"pickled\" the data for a later use. \"Pickling\" is basically exporting a Python object as a file; you can \"unpickle\" the file later to use it in Python again ([a more technical explanation](https://python.readthedocs.io/en/stable/library/pickle.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "data = {}\n",
    "for url in hyperlinks:\n",
    "    title, text = scrape_american_presidency_doc_content(\"https://www.presidency.ucsb.edu\" + url)\n",
    "    data[title] = text\n",
    "    time.sleep(1)\n",
    "    \n",
    "# For storing the data for later use\n",
    "with open('data_raw.p', 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_raw.p', 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the scraping go well? Let's take a look at our full data. Our data is in the form of a \"dictionary\", in which \"keys\" are the titles and \"values\" are the body texts. We may call it a \"corpus\", which means a collection of texts.\n",
    "\n",
    "The cell below shows us all the keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying a key, we can look up the value corresponding to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['2020 Democratic Party Platform']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: What if you want to save the data as files that can be opened outside of Python? You can save them as .txt file below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "\n",
    "for title, text in data.items():\n",
    "    with open('./data/'+title+'.txt', 'w', encoding='utf8') as file:\n",
    "        for paragraph in text:\n",
    "            file.write(paragraph + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data: A Toy Example\n",
    "\n",
    "Before we work on the actual data, let's take a look at a toy example because it's easier to see what's happening with a smaller dataset. I made a small dictionary below, which contains three haikus (very short Japanese poems) about spring. It has the same structure as our data we scraped above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_haiku = {\n",
    "    'Yosa Buson': 'The spring rain. / Talking and passing / The straw rain‐cape and umbrella.',\n",
    "    'Murakami Kijo': 'The spring rain, / I definitely saw / The fay of stone.',\n",
    "    'Mizuhara Shuoshi': 'The bush warbler. / The rain wouldn\\'t let up. / The travel clothes.'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this corpus, We want to make the table below. It is called \"document-term matrix\", a simple way to represent human-generated text in a form that a computer can read. \n",
    "\n",
    "This table stores the word (term) counts in each document. Each column label is a word, each row corresponds to a poem, and the numbers are the number of occurrences of the words in each poem.\n",
    "<img src=\"images/dtm_example.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spring_haiku['Yosa Buson']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first want to convert the texts into a list of words. There are many ways to do so, but let' try the tokenizer function included in NLTK (Natural Language Toolkit) library. (In a loose sense, a token is like a word, and to tokenize means to break down texts into tokens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "word_tokenize(spring_haiku['Yosa Buson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the slashes and periods are treated as individual tokens by the NLTK tokenizer. There is no fixed answer to how to treat punctuations and the decision depends on your goal and interests, but they often get removed when cleaning text data. Let's say we decided to remove punctuations.\n",
    "\n",
    "In addition, let's try to normalize the cases by making all the letters to lower-case letters. Like punctuations, you could leave the letter cases as they are, but normalizing them is also a common text data cleaning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def non_punct_or_punct(char):\n",
    "    '''\n",
    "    Returns True if the character is NOT a punctuation; False otherwise. \n",
    "    '''\n",
    "    if char in punctuation:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    '''\n",
    "    Cleans and tokenizes the text and returns the tokens as a list.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = filter(non_punct_or_punct, tokens)\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_and_tokenize(spring_haiku['Yosa Buson'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a document-term matrix. Scikit-learn's CountVectorizer makes it very easy to build one.\n",
    "\n",
    "In addition to a corpus, I can pass a function to use as a tokenizer and a list of stop words. Stop words are extremely common words that usually do not have much value in data analysis (e.g., the, and, about). There are other parameters you could pass or change, and you can see them by pressing Shift+Tab with the cursor placed on the first line of the function.\n",
    "\n",
    "To process my small haiku corpus, I passed the clean_and_tokenize function I wrote as a tokenizer for CountVectorizer to use. I did not pass any stop words this time because my corpus is so small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def make_document_term_matrix(corpus, stop_words, tokenizer):\n",
    "    '''\n",
    "    Receives a corpus in a dictionary format and returns a document-term matrix made based on it.\n",
    "    \n",
    "    Parameters\n",
    "        corpus: dictionary\n",
    "        stop_words: list, list of stop words to remove\n",
    "        tokenizer: function, function that tokenizes the corpus (if None, use the default tokenizer of scikit-learn)\n",
    "    \n",
    "    Returns     \n",
    "        doc_term_mat: pandas dataframe, a document-term matrix\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words, tokenizer=tokenizer)\n",
    "    vectors = vectorizer.fit_transform(corpus.values())\n",
    "    doc_term_mat = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    doc_term_mat.index = corpus.keys()\n",
    "    return doc_term_mat\n",
    "\n",
    "dt_matrix_haiku = make_document_term_matrix(spring_haiku, None, clean_and_tokenize)\n",
    "dt_matrix_haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about stop words: there are several standard stop words lists you could use, but like punctuations, lowercasing, and any other cleaning steps, the decision about which words are considered as stop words should be based on your data and goal. The next cell shows you the list of stop words included in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "print(ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows you the list of stop words included in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data: Party Platforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to come back to the party platform corpus. Before proceeding to create a document-term matrix, let's take a look at our data again. The next cell shows the title of the first document in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body text of the first document looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have a list of strings as a value of my dictionary. Why? When I scraped the data, the text was split into paragraphs and I put them into a list. I'd like to merge them into one big string because it's easier to deal with. Let's do it and see the merged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, text in data.items():\n",
    "    data[title] = ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(data.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our corpus is ready to be preprocessed into a document-term matrix. This time I tell the CountVectorizer to use scikit-learn's standard stop-word list and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_mat = make_document_term_matrix(data, 'english', None)\n",
    "dt_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Word Clouds to Examine the Most Frequent Words\n",
    "\n",
    "We converted a bunch of texts into a bunch of numbers that a computer can read. Now it’s time for some data analysis!\n",
    "\n",
    "### Word Clouds\n",
    "\n",
    "We can do so many things with this document-term matrix, but since this is a short tutorial, I'll show you how to do a simple exploratory data analysis by visualizing them as word clouds. Word clouds are an intuitive way to visulize texts. By looking at word clouds, we could decide which aspect of the data we want to dig deeper into and develop our research question further.\n",
    "\n",
    "### Comparison Between the Parties\n",
    "\n",
    "How does the language of the Democratic Party differ from that of the Republican Party? Let's represent the party platforms published in 2012 as word clouds and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_word_clouds_dem_rep(data, year):\n",
    "    '''\n",
    "    Draws a pair of word clouds, one for Democratic Party Platform and one for Republican Party Platform.\n",
    "    Saves it as a .png file.\n",
    "    \n",
    "    Parameters\n",
    "        data: a document-term matrix\n",
    "        year: int, the year of publication\n",
    "    '''\n",
    "    wc = WordCloud(max_words=100, background_color='white', colormap='Dark2', max_font_size=150, random_state=1)\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    num = 0\n",
    "    for title in [' Democratic Party Platform', ' Republican Party Platform']:\n",
    "        data_dict = dict(zip(data.columns, data.loc[str(year)+title]))\n",
    "        wc.generate_from_frequencies(data_dict)\n",
    "        num += 1\n",
    "        \n",
    "        plt.subplot(1, 2, num)\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(year)+title, fontsize=20)\n",
    "        \n",
    "    # Save the plot as a file; instead of '.png', other formats such as '.pdf' or '.jpg' can be used\n",
    "    plt.savefig(\"party_platforms_\"+str(year)+'.png')\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_word_clouds_dem_rep(dt_mat, 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Counts vs. tf-idf\n",
    "\n",
    "There are many overlapping words between the two parties such as \"american\" and \"states\". It’s not surprising that there are common words that usually appear in party platforms. If they are not emphasized this much, it might have been easier for us to see the more interesting words that actually distinguish the two parties.\n",
    "\n",
    "We cau solve this issue by using tf–idf (term frequency–inverse document frequency) instead of word counts. Tf-idf is a measurement that reflects how important a word is to a document in a corpus.\n",
    "\n",
    "<br><center><b>tf (term frequency)</b>: raw term count (word count) / the total number of terms (words) in the document<br> \n",
    "    <b>idf (inverse document frequency)</b>: log (the total number of documents / the number of documents in which the term appears)<br>\n",
    "    <b>tf-idf</b>: <b>tf</b> * <b>idf</b></center><br>\n",
    "\n",
    "[More details about tf-idf (Wikipedia)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Term_frequency%E2%80%93Inverse_document_frequency)\n",
    "\n",
    "We could make a document-term matrix that contains tf-idf scores rather than word counts with scikit-learn's TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def make_tfidf_document_term_matrix(corpus, stop_words, tokenizer):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words, tokenizer=tokenizer)\n",
    "    vectors = vectorizer.fit_transform(corpus.values())\n",
    "    data_tfidf_mat = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names())\n",
    "    data_tfidf_mat.index = corpus.keys()\n",
    "    return data_tfidf_mat\n",
    "\n",
    "tfidf_dt_mat = make_tfidf_document_term_matrix(data, 'english', None)\n",
    "tfidf_dt_mat.head(3) # show only the first three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_word_clouds_dem_rep(tfidf_dt_mat, 2012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison Between the Time Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_word_clouds_older_newer(data, party, year1, year2):\n",
    "    '''\n",
    "    Draws a pair of word clouds, one for Democratic Party Platform and one for Republican Party Platform.\n",
    "    Saves it as a .png file.\n",
    "    \n",
    "    Parameters\n",
    "        data: a document-term matrix\n",
    "        party: string, either dem or rep\n",
    "        year1: int, year of publication\n",
    "        year2: int, year of publication to compare against year 1\n",
    "    '''\n",
    "    wc = WordCloud(max_words=100, background_color='white', colormap='Dark2', max_font_size=150, random_state=1)\n",
    "    plt.rcParams['figure.figsize'] = [20, 10]\n",
    "    num = 0\n",
    "    \n",
    "    title = ''\n",
    "    if party == 'dem':\n",
    "        title = ' Democratic Party Platform'\n",
    "    else:\n",
    "        title = ' Republican Party Platform'\n",
    "    \n",
    "    for year in [year1, year2]:\n",
    "        data_dict = dict(zip(data.columns, data.loc[str(year)+title]))\n",
    "        wc.generate_from_frequencies(data_dict)\n",
    "        num += 1\n",
    "        \n",
    "        plt.subplot(1, 2, num)\n",
    "        plt.imshow(wc, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(str(year)+title, fontsize=20)\n",
    "        \n",
    "    # Save the plot as a file; instead of '.png', other formats such as '.pdf' or '.jpg' can be used\n",
    "    plt.savefig(\"party_platforms_\"+str(year)+'.png')\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "    \n",
    "draw_word_clouds_older_newer(tfidf_dt_mat, 'dem', 1940, 2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish, let's store the document-term matrices we have made for a later use. If we have another tutorial, we could load them in a new Jupyter Notebook and do more advanced data analysis with various machine learning methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For storing the data for later use\n",
    "with open('dtm.p', 'wb') as file:\n",
    "    pickle.dump(dt_mat, file)\n",
    "\n",
    "with open('tfidf_dtm.p', 'wb') as file:\n",
    "    pickle.dump(tfidf_dt_mat, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
